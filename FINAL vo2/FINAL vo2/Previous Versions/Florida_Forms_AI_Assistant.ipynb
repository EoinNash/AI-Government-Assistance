{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Florida Government Forms AI Assistant\n",
    "## Intro to AI - Final Project Implementation\n",
    "\n",
    "**Team Members:**\n",
    "- Carlecia Gordon (Carly) - Computer Science\n",
    "- Giovanny Victome (jjviscane) - Computer Engineering\n",
    "- Raptor - Computer Engineering\n",
    "- Captain capital PSTL - Computer Science\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "This project implements an AI system that:\n",
    "1. Classifies government form images using a **Convolutional Neural Network (CNN)**\n",
    "2. Answers text queries about Florida DMV/permit procedures using a **Multi-Layer Perceptron (MLP)**\n",
    "\n",
    "### Key AI Concepts Demonstrated:\n",
    "- **INPUT LAYER**: Receives preprocessed data\n",
    "- **CONVOLUTIONAL LAYERS**: Feature extraction from images\n",
    "- **POOLING LAYERS**: Dimensionality reduction\n",
    "- **HIDDEN LAYERS (MLP)**: High-level reasoning\n",
    "- **OUTPUT LAYER**: Classification probabilities\n",
    "- **BACKPROPAGATION**: Learning algorithm\n",
    "- **LOSS FUNCTION**: Error measurement"
   ],
   "metadata": {
    "id": "title"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# WEEK 1: Foundation & Data Collection\n",
    "---"
   ],
   "metadata": {
    "id": "week1_title"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1A: Initial Setup & Infrastructure\n",
    "### Google Colab Environment Setup (Giovanny - Computer Engineering)"
   ],
   "metadata": {
    "id": "setup_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Mount Google Drive and create project structure\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create organized folder structure\n",
    "!mkdir -p '/content/drive/MyDrive/AI_Project/data/raw_forms'\n",
    "!mkdir -p '/content/drive/MyDrive/AI_Project/data/processed'\n",
    "!mkdir -p '/content/drive/MyDrive/AI_Project/models'\n",
    "!mkdir -p '/content/drive/MyDrive/AI_Project/results'\n",
    "!mkdir -p '/content/drive/MyDrive/AI_Project/presentation'\n",
    "\n",
    "print(\"‚úÖ Project structure created\")"
   ],
   "metadata": {
    "id": "mount_drive"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Install additional packages\n",
    "!pip install -q tensorflow opencv-python pillow scikit-learn pdf2image\n",
    "!apt-get install -q poppler-utils\n",
    "\n",
    "print(\"‚úÖ Packages installed\")"
   ],
   "metadata": {
    "id": "install_packages"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Core imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ],
   "metadata": {
    "id": "imports"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1B: Database Design & Implementation\n",
    "### SQLite Database Schema (Carly - Computer Science)"
   ],
   "metadata": {
    "id": "database_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Connect to database\n",
    "db_path = '/content/drive/MyDrive/AI_Project/florida_forms.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"‚úÖ Database connection established\")"
   ],
   "metadata": {
    "id": "db_connect"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create forms table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS forms (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    form_number TEXT NOT NULL,\n",
    "    form_name TEXT NOT NULL,\n",
    "    category TEXT NOT NULL,\n",
    "    url TEXT,\n",
    "    description TEXT,\n",
    "    requirements TEXT,\n",
    "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "''')\n",
    "\n",
    "# Create index for faster queries\n",
    "cursor.execute('''\n",
    "CREATE INDEX IF NOT EXISTS idx_category ON forms(category)\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Forms table created\")"
   ],
   "metadata": {
    "id": "create_forms_table"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create queries table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS queries (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    question TEXT NOT NULL,\n",
    "    category TEXT NOT NULL,\n",
    "    intent TEXT,\n",
    "    answer TEXT,\n",
    "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE INDEX IF NOT EXISTS idx_query_category ON queries(category)\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Queries table created\")"
   ],
   "metadata": {
    "id": "create_queries_table"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Florida DMV forms data\n",
    "forms_data = [\n",
    "    ('HSMV 82040', 'Application for Duplicate/Renewal', 'License',\n",
    "     'https://www.flhsmv.gov/pdf/forms/82040.pdf',\n",
    "     'Renew or replace Florida driver license or ID card',\n",
    "     'Proof of identity, residency, SSN'),\n",
    "    \n",
    "    ('HSMV 82042', 'Medical Examination Report', 'License',\n",
    "     'https://www.flhsmv.gov/pdf/forms/82042.pdf',\n",
    "     'Medical certification for commercial license',\n",
    "     'Physical exam by certified examiner'),\n",
    "    \n",
    "    ('HSMV 82040', 'Vehicle Registration Renewal', 'Registration',\n",
    "     'https://www.flhsmv.gov/pdf/forms/82040.pdf',\n",
    "     'Renew vehicle registration',\n",
    "     'Current registration, insurance proof'),\n",
    "    \n",
    "    ('HSMV 82041', 'Application for Certificate of Title', 'Title',\n",
    "     'https://www.flhsmv.gov/pdf/forms/82041.pdf',\n",
    "     'Apply for vehicle title',\n",
    "     'Bill of sale, previous title, ID'),\n",
    "    \n",
    "    ('HSMV 83330', 'Learner License Application', 'Permit',\n",
    "     'https://www.flhsmv.gov/pdf/forms/83330.pdf',\n",
    "     'Apply for learners permit',\n",
    "     'Parental consent if under 18, ID documents'),\n",
    "    \n",
    "    ('HSMV 71054', 'Skill Test Waiver', 'License',\n",
    "     'https://www.flhsmv.gov/pdf/forms/71054.pdf',\n",
    "     'Waive driving test with approved course',\n",
    "     'Completion certificate from driving school'),\n",
    "    \n",
    "    ('HSMV 82053', 'Fast Title Application', 'Title',\n",
    "     'https://www.flhsmv.gov/pdf/forms/82053.pdf',\n",
    "     'Expedited title processing',\n",
    "     'Standard title requirements plus expedite fee'),\n",
    "    \n",
    "    ('HSMV 71081', 'Insurance Affidavit', 'Registration',\n",
    "     'https://www.flhsmv.gov/pdf/forms/71081.pdf',\n",
    "     'Proof of insurance for registration',\n",
    "     'Valid insurance policy information'),\n",
    "    \n",
    "    ('HSMV 82042', 'ID Card Application', 'ID',\n",
    "     'https://www.flhsmv.gov/pdf/forms/82042.pdf',\n",
    "     'Apply for Florida identification card',\n",
    "     'Proof of identity, residency, SSN'),\n",
    "    \n",
    "    ('HSMV 83045', 'Power of Attorney', 'General',\n",
    "     'https://www.flhsmv.gov/pdf/forms/83045.pdf',\n",
    "     'Authorize representative for DMV transactions',\n",
    "     'Notarized signature, representative ID')\n",
    "]\n",
    "\n",
    "cursor.executemany('''\n",
    "INSERT INTO forms (form_number, form_name, category, url, description, requirements)\n",
    "VALUES (?, ?, ?, ?, ?, ?)\n",
    "''', forms_data)\n",
    "\n",
    "conn.commit()\n",
    "print(f\"‚úÖ Inserted {len(forms_data)} forms into database\")\n",
    "\n",
    "# Verify insertion\n",
    "cursor.execute(\"SELECT category, COUNT(*) FROM forms GROUP BY category\")\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  - {row[0]}: {row[1]} forms\")"
   ],
   "metadata": {
    "id": "populate_forms"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1C: Form Image Collection\n",
    "### Download and Convert PDFs (Raptor - Computer Engineering)"
   ],
   "metadata": {
    "id": "image_collection_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def download_form(url, save_path):\n",
    "    \"\"\"Download PDF form from URL\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Download all forms from database\n",
    "cursor.execute(\"SELECT form_number, url, category FROM forms\")\n",
    "forms_list = cursor.fetchall()\n",
    "\n",
    "raw_forms_dir = '/content/drive/MyDrive/AI_Project/data/raw_forms'\n",
    "\n",
    "for form_num, url, category in forms_list:\n",
    "    if url:\n",
    "        filename = f\"{category}__{form_num.replace(' ', '_')}.pdf\"\n",
    "        save_path = os.path.join(raw_forms_dir, filename)\n",
    "        \n",
    "        if download_form(url, save_path):\n",
    "            print(f\"‚úÖ Downloaded: {filename}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed: {filename}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total forms downloaded: {len(os.listdir(raw_forms_dir))}\")"
   ],
   "metadata": {
    "id": "download_forms"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def pdf_to_image(pdf_path, output_dir, category):\n",
    "    \"\"\"Convert first page of PDF to PNG image\"\"\"\n",
    "    try:\n",
    "        # Convert first page only\n",
    "        images = convert_from_path(pdf_path, first_page=1, last_page=1, dpi=150)\n",
    "        \n",
    "        # Save as PNG\n",
    "        filename = os.path.basename(pdf_path).replace('.pdf', '.png')\n",
    "        output_path = os.path.join(output_dir, category, filename)\n",
    "        \n",
    "        os.makedirs(os.path.join(output_dir, category), exist_ok=True)\n",
    "        images[0].save(output_path, 'PNG')\n",
    "        \n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Convert all PDFs\n",
    "images_dir = '/content/drive/MyDrive/AI_Project/data/processed/images'\n",
    "\n",
    "for pdf_file in os.listdir(raw_forms_dir):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        category = pdf_file.split('__')[0]\n",
    "        pdf_path = os.path.join(raw_forms_dir, pdf_file)\n",
    "        \n",
    "        img_path = pdf_to_image(pdf_path, images_dir, category)\n",
    "        if img_path:\n",
    "            print(f\"‚úÖ Converted: {pdf_file}\")\n",
    "\n",
    "print(\"\\n‚úÖ PDF to image conversion complete\")"
   ],
   "metadata": {
    "id": "convert_pdfs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import ImageEnhance, ImageOps\n",
    "import random\n",
    "\n",
    "def augment_image(image_path, output_dir, num_augmentations=10):\n",
    "    \"\"\"\n",
    "    Create augmented versions of form image\n",
    "    Augmentations: rotation, brightness, contrast, flip\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    \n",
    "    augmented_paths = []\n",
    "    \n",
    "    for i in range(num_augmentations):\n",
    "        aug_img = img.copy()\n",
    "        \n",
    "        # Random rotation (-5 to 5 degrees)\n",
    "        angle = random.uniform(-5, 5)\n",
    "        aug_img = aug_img.rotate(angle, fillcolor='white')\n",
    "        \n",
    "        # Random brightness (0.8 to 1.2)\n",
    "        brightness = ImageEnhance.Brightness(aug_img)\n",
    "        aug_img = brightness.enhance(random.uniform(0.8, 1.2))\n",
    "        \n",
    "        # Random contrast (0.9 to 1.1)\n",
    "        contrast = ImageEnhance.Contrast(aug_img)\n",
    "        aug_img = contrast.enhance(random.uniform(0.9, 1.1))\n",
    "        \n",
    "        # Save augmented image\n",
    "        aug_filename = f\"{base_name}_aug{i}.png\"\n",
    "        aug_path = os.path.join(output_dir, aug_filename)\n",
    "        aug_img.save(aug_path)\n",
    "        \n",
    "        augmented_paths.append(aug_path)\n",
    "    \n",
    "    return augmented_paths\n",
    "\n",
    "# Augment all images\n",
    "augmented_dir = '/content/drive/MyDrive/AI_Project/data/processed/augmented'\n",
    "\n",
    "total_images = 0\n",
    "for category in os.listdir(images_dir):\n",
    "    category_path = os.path.join(images_dir, category)\n",
    "    \n",
    "    if os.path.isdir(category_path):\n",
    "        \n",
    "        # Create category subfolder in augmented dir\n",
    "        aug_category_dir = os.path.join(augmented_dir, category)\n",
    "        os.makedirs(aug_category_dir, exist_ok=True)\n",
    "        \n",
    "        for img_file in os.listdir(category_path):\n",
    "            img_path = os.path.join(category_path, img_file)\n",
    "            \n",
    "            # Copy original\n",
    "            original_copy = os.path.join(aug_category_dir, img_file)\n",
    "            Image.open(img_path).save(original_copy)\n",
    "            \n",
    "            # Create augmentations\n",
    "            aug_paths = augment_image(img_path, aug_category_dir, num_augmentations=14)\n",
    "            total_images += len(aug_paths) + 1  # +1 for original\n",
    "            \n",
    "            print(f\"‚úÖ Augmented: {img_file} ‚Üí {len(aug_paths)} versions\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total dataset size: {total_images} images\")\n",
    "print(f\"‚úÖ Target: ~150 images (15 per category)\")"
   ],
   "metadata": {
    "id": "augment_images"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1D: Image Preprocessing Pipeline\n",
    "### Create preprocessing functions (Giovanny - Computer Engineering)"
   ],
   "metadata": {
    "id": "preprocessing_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_image(image_path, target_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Preprocess form images for CNN input\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        target_size: Desired output dimensions (height, width)\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed numpy array normalized to [0, 1]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read image in grayscale\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not read image: {image_path}\")\n",
    "        \n",
    "        # Resize to target size\n",
    "        img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Normalize pixel values to [0, 1]\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Reshape to add channel dimension (H, W, 1)\n",
    "        img = img.reshape(target_size[0], target_size[1], 1)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test preprocessing\n",
    "test_categories = [d for d in os.listdir(augmented_dir) if os.path.isdir(os.path.join(augmented_dir, d))]\n",
    "if test_categories:\n",
    "    test_category = test_categories[0]\n",
    "    test_imgs = os.listdir(os.path.join(augmented_dir, test_category))\n",
    "    if test_imgs:\n",
    "        test_img_path = os.path.join(augmented_dir, test_category, test_imgs[0])\n",
    "        test_img = preprocess_image(test_img_path)\n",
    "        \n",
    "        print(f\"‚úÖ Preprocessing test successful\")\n",
    "        print(f\"  - Input: {test_img_path}\")\n",
    "        print(f\"  - Output shape: {test_img.shape}\")\n",
    "        print(f\"  - Value range: [{test_img.min():.2f}, {test_img.max():.2f}]\")"
   ],
   "metadata": {
    "id": "preprocess_function"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_dataset(data_dir, target_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Load entire dataset from directory structure\n",
    "    \n",
    "    Directory structure expected:\n",
    "        data_dir/\n",
    "            Category1/\n",
    "                image1.png\n",
    "                image2.png\n",
    "            Category2/\n",
    "                image1.png\n",
    "    \n",
    "    Returns:\n",
    "        X: numpy array of images (N, H, W, 1)\n",
    "        y: numpy array of labels (N,)\n",
    "        categories: list of category names\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    categories = sorted(os.listdir(data_dir))\n",
    "    \n",
    "    # Remove non-directory items\n",
    "    categories = [c for c in categories if os.path.isdir(os.path.join(data_dir, c))]\n",
    "    \n",
    "    print(f\"Loading dataset from {len(categories)} categories...\")\n",
    "    \n",
    "    for label_idx, category in enumerate(categories):\n",
    "        category_path = os.path.join(data_dir, category)\n",
    "        image_files = [f for f in os.listdir(category_path) \n",
    "                      if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        print(f\"  Loading {category}: {len(image_files)} images\", end='')\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(category_path, img_file)\n",
    "            img = preprocess_image(img_path, target_size)\n",
    "            \n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                labels.append(label_idx)\n",
    "        \n",
    "        print(f\" ‚Üí {len([l for l in labels if l == label_idx])} loaded\")\n",
    "    \n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset loaded successfully\")\n",
    "    print(f\"  - Total images: {len(X)}\")\n",
    "    print(f\"  - Image shape: {X[0].shape}\")\n",
    "    print(f\"  - Categories: {categories}\")\n",
    "    print(f\"  - Label distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    return X, y, categories\n",
    "\n",
    "# Load complete dataset\n",
    "X_full, y_full, category_names = load_dataset(augmented_dir)"
   ],
   "metadata": {
    "id": "load_dataset"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# First split: separate test set (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_full, y_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_full\n",
    ")\n",
    "\n",
    "# Second split: separate validation from training (20% of remaining)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.25,  # 0.25 * 0.8 = 0.2 of total\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Dataset Split:\")\n",
    "print(f\"  - Training: {len(X_train)} images ({len(X_train)/len(X_full)*100:.1f}%)\")\n",
    "print(f\"  - Validation: {len(X_val)} images ({len(X_val)/len(X_full)*100:.1f}%)\")\n",
    "print(f\"  - Testing: {len(X_test)} images ({len(X_test)/len(X_full)*100:.1f}%)\")\n",
    "\n",
    "# Save preprocessed datasets\n",
    "np.save('/content/drive/MyDrive/AI_Project/data/processed/X_train.npy', X_train)\n",
    "np.save('/content/drive/MyDrive/AI_Project/data/processed/X_val.npy', X_val)\n",
    "np.save('/content/drive/MyDrive/AI_Project/data/processed/X_test.npy', X_test)\n",
    "np.save('/content/drive/MyDrive/AI_Project/data/processed/y_train.npy', y_train)\n",
    "np.save('/content/drive/MyDrive/AI_Project/data/processed/y_val.npy', y_val)\n",
    "np.save('/content/drive/MyDrive/AI_Project/data/processed/y_test.npy', y_test)\n",
    "\n",
    "# Save category names\n",
    "with open('/content/drive/MyDrive/AI_Project/data/processed/categories.pkl', 'wb') as f:\n",
    "    pickle.dump(category_names, f)\n",
    "\n",
    "print(\"\\n‚úÖ Preprocessed datasets saved to Drive\")"
   ],
   "metadata": {
    "id": "train_test_split"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize sample images from each category\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Sample Forms by Category', fontsize=16)\n",
    "\n",
    "for idx, category in enumerate(category_names):\n",
    "    # Get first image from this category\n",
    "    category_indices = np.where(y_full == idx)[0]\n",
    "    if len(category_indices) > 0:\n",
    "        sample_img = X_full[category_indices[0]]\n",
    "        \n",
    "        ax = axes[idx // 5, idx % 5]\n",
    "        ax.imshow(sample_img.squeeze(), cmap='gray')\n",
    "        ax.set_title(category)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/AI_Project/results/sample_forms.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sample visualization saved for presentation\")"
   ],
   "metadata": {
    "id": "visualize_samples"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# WEEK 2: CNN Model Development\n",
    "---"
   ],
   "metadata": {
    "id": "week2_title"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2A: CNN Architecture Design\n",
    "### Comprehensive CNN Model (Captain capital - Computer Science)"
   ],
   "metadata": {
    "id": "cnn_architecture_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def create_cnn_model(input_shape=(128, 128, 1), num_classes=5):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for form classification\n",
    "    \n",
    "    Architecture demonstrates:\n",
    "    - INPUT LAYER: Receives preprocessed images\n",
    "    - CONVOLUTIONAL LAYERS: Feature extraction from images\n",
    "    - POOLING LAYERS: Dimensionality reduction\n",
    "    - HIDDEN LAYERS (MLP): High-level reasoning\n",
    "    - OUTPUT LAYER: Classification probabilities\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Image dimensions (height, width, channels)\n",
    "        num_classes: Number of form categories\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # INPUT LAYER\n",
    "        layers.Input(shape=input_shape, name='input'),\n",
    "        \n",
    "        # CONVOLUTIONAL BLOCK 1\n",
    "        # Purpose: Detect low-level features (edges, lines, corners)\n",
    "        # These are building blocks that appear in form layouts\n",
    "        layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='conv1'),\n",
    "        layers.BatchNormalization(name='bn1'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool1'),  # 128x128 ‚Üí 64x64\n",
    "        layers.Dropout(0.25, name='dropout1'),\n",
    "        \n",
    "        # CONVOLUTIONAL BLOCK 2\n",
    "        # Purpose: Detect mid-level features (form sections, text blocks, logos)\n",
    "        # Combines low-level features into meaningful patterns\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv2'),\n",
    "        layers.BatchNormalization(name='bn2'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool2'),  # 64x64 ‚Üí 32x32\n",
    "        layers.Dropout(0.25, name='dropout2'),\n",
    "        \n",
    "        # CONVOLUTIONAL BLOCK 3\n",
    "        # Purpose: Detect high-level features (overall form structure, layout)\n",
    "        # Recognizes complete form patterns\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv3'),\n",
    "        layers.BatchNormalization(name='bn3'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool3'),  # 32x32 ‚Üí 16x16\n",
    "        layers.Dropout(0.25, name='dropout3'),\n",
    "        \n",
    "        # Flatten feature maps to 1D vector\n",
    "        layers.Flatten(name='flatten'),\n",
    "        \n",
    "        # HIDDEN LAYERS (Multi-Layer Perceptron)\n",
    "        # Purpose: Learn complex combinations of visual features\n",
    "        # These fully-connected layers perform classification reasoning\n",
    "        layers.Dense(128, activation='relu', name='hidden1'),\n",
    "        layers.BatchNormalization(name='bn4'),\n",
    "        layers.Dropout(0.4, name='dropout4'),\n",
    "        \n",
    "        layers.Dense(64, activation='relu', name='hidden2'),\n",
    "        layers.Dropout(0.4, name='dropout5'),\n",
    "        \n",
    "        # OUTPUT LAYER\n",
    "        # Softmax activation provides probability distribution over classes\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model instance\n",
    "cnn_model = create_cnn_model(num_classes=len(category_names))\n",
    "\n",
    "# Display architecture\n",
    "print(\"=\"*60)\n",
    "print(\"CNN MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "cnn_model.summary()\n",
    "\n",
    "# Calculate model parameters\n",
    "total_params = cnn_model.count_params()\n",
    "print(f\"\\n‚úÖ Total parameters: {total_params:,}\")"
   ],
   "metadata": {
    "id": "create_cnn_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Why Convolutional Layers are Important\n",
    "\n",
    "**1. AUTOMATIC FEATURE LEARNING**\n",
    "- Traditional approach: Manually program what edges/shapes to look for\n",
    "- CNN approach: Learns optimal features automatically from data\n",
    "- Example: CNN discovers which form patterns distinguish licenses from permits\n",
    "\n",
    "**2. PARAMETER SHARING**\n",
    "- Same convolutional filter applied across entire image\n",
    "- Detects features regardless of position in form\n",
    "- Dramatically reduces parameters compared to fully-connected layers\n",
    "- Example: 3x3 filter has only 9 weights, but scans entire 128x128 image\n",
    "\n",
    "**3. TRANSLATION INVARIANCE**\n",
    "- Form slightly shifted? CNN still recognizes it\n",
    "- Logo in different corner? CNN adapts\n",
    "- Pooling layers enhance this property\n",
    "\n",
    "**4. HIERARCHICAL LEARNING**\n",
    "- Layer 1: Edges and simple patterns\n",
    "- Layer 2: Combinations ‚Üí form sections\n",
    "- Layer 3: Full layouts ‚Üí form type identification\n",
    "\n",
    "**5. EFFICIENCY FOR IMAGES**\n",
    "- Exploits 2D structure of images\n",
    "- More efficient than treating image as flat vector\n",
    "- In our project: 128x128=16,384 pixels would need millions of weights in fully-connected network, but CNN uses only thousands"
   ],
   "metadata": {
    "id": "conv_explanation"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Why Pooling is Important\n",
    "\n",
    "**1. DIMENSIONALITY REDUCTION**\n",
    "- Reduces spatial size: 64x64 ‚Üí 32x32\n",
    "- Fewer computations in subsequent layers\n",
    "- Prevents overfitting by reducing parameters\n",
    "\n",
    "**2. MAX POOLING (2x2) OPERATION**\n",
    "- Divides feature map into 2x2 regions\n",
    "- Keeps maximum value from each region\n",
    "- Preserves strongest activations (most important features)\n",
    "\n",
    "**3. TRANSLATION INVARIANCE**\n",
    "- Small shifts in input don't change pooled output\n",
    "- Form slightly off-center? Still same pooled features\n",
    "- Makes model robust to minor variations\n",
    "\n",
    "**4. COMPUTATIONAL EFFICIENCY**\n",
    "- Each pooling layer reduces feature map size by 75%\n",
    "- Speeds up training and inference\n",
    "- In our model: 128x128 ‚Üí 64x64 ‚Üí 32x32 ‚Üí 16x16\n",
    "\n",
    "**5. RETAINS IMPORTANT INFORMATION**\n",
    "- Max pooling keeps strongest signals\n",
    "- Discards redundant spatial information\n",
    "- Focuses on \"what\" features exist, not exact \"where\""
   ],
   "metadata": {
    "id": "pooling_explanation"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# LOSS FUNCTION: Categorical Cross-Entropy\n",
    "cnn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ CNN model compiled with categorical cross-entropy loss\")"
   ],
   "metadata": {
    "id": "compile_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss Function Explanation\n",
    "\n",
    "**Categorical Cross-Entropy Loss:**\n",
    "- Measures difference between predicted probabilities and true labels\n",
    "- Formula: Loss = -Œ£(y_true * log(y_pred))\n",
    "- Low loss = good predictions, High loss = poor predictions\n",
    "- Used for multi-class classification (License, Registration, Title, etc.)\n",
    "\n",
    "**Why this loss function:**\n",
    "- Softmax output layer produces probabilities summing to 1.0\n",
    "- Cross-entropy penalizes confident wrong predictions heavily\n",
    "- Works well with backpropagation for gradient calculation\n",
    "\n",
    "**Example:**\n",
    "- True label: License (category 0)\n",
    "- Prediction: [0.8, 0.1, 0.05, 0.03, 0.02] ‚Üí Low loss (confident & correct)\n",
    "- Prediction: [0.2, 0.3, 0.3, 0.1, 0.1] ‚Üí High loss (uncertain)\n",
    "- Prediction: [0.1, 0.7, 0.1, 0.05, 0.05] ‚Üí Very high loss (confident & wrong)"
   ],
   "metadata": {
    "id": "loss_explanation"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2B: Model Training with Backpropagation\n",
    "### Training Setup (Captain capital - Computer Science)"
   ],
   "metadata": {
    "id": "training_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load preprocessed datasets\n",
    "X_train = np.load('/content/drive/MyDrive/AI_Project/data/processed/X_train.npy')\n",
    "X_val = np.load('/content/drive/MyDrive/AI_Project/data/processed/X_val.npy')\n",
    "X_test = np.load('/content/drive/MyDrive/AI_Project/data/processed/X_test.npy')\n",
    "y_train = np.load('/content/drive/MyDrive/AI_Project/data/processed/y_train.npy')\n",
    "y_val = np.load('/content/drive/MyDrive/AI_Project/data/processed/y_val.npy')\n",
    "y_test = np.load('/content/drive/MyDrive/AI_Project/data/processed/y_test.npy')\n",
    "\n",
    "with open('/content/drive/MyDrive/AI_Project/data/processed/categories.pkl', 'rb') as f:\n",
    "    category_names = pickle.load(f)\n",
    "\n",
    "print(\"Dataset loaded:\")\n",
    "print(f\"  Training: {X_train.shape[0]} images\")\n",
    "print(f\"  Validation: {X_val.shape[0]} images\")\n",
    "print(f\"  Testing: {X_test.shape[0]} images\")\n",
    "print(f\"  Categories: {category_names}\")"
   ],
   "metadata": {
    "id": "load_data_training"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Save best model based on validation accuracy\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='/content/drive/MyDrive/AI_Project/models/best_cnn_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Stop training if validation loss doesn't improve\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reduce learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, early_stop, reduce_lr]\n",
    "\n",
    "print(\"‚úÖ Training callbacks configured\")"
   ],
   "metadata": {
    "id": "training_callbacks"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the CNN model\n",
    "# BACKPROPAGATION happens automatically during training!\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING CNN TRAINING\")\n",
    "print(\"Backpropagation will adjust weights to minimize loss\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ],
   "metadata": {
    "id": "train_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How Backpropagation Works\n",
    "\n",
    "**Step-by-Step Process:**\n",
    "\n",
    "1. **Forward Pass**: Input ‚Üí through all layers ‚Üí prediction\n",
    "2. **Calculate Loss**: Compare prediction to correct answer using loss function\n",
    "3. **Backward Pass**: Calculate how much each weight contributed to error\n",
    "4. **Update Weights**: Adjust weights to reduce error (gradient descent)\n",
    "5. **Repeat**: Do this thousands of times until model is accurate\n",
    "\n",
    "**In our CNN:**\n",
    "- If form classified wrong, backprop adjusts convolutional filters\n",
    "- Gradients flow backward through all layers\n",
    "- Adam optimizer determines how much to adjust each weight\n",
    "- Learning rate controls size of weight updates\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Gradient**: Direction to adjust weight to reduce loss\n",
    "- **Learning Rate**: How big each adjustment step is\n",
    "- **Epoch**: One complete pass through training data\n",
    "- **Batch**: Subset of data processed before updating weights"
   ],
   "metadata": {
    "id": "backprop_explanation"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0].set_title('Model Accuracy Over Time')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Training Loss')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[1].set_title('Model Loss Over Time (Backpropagation Minimizes This)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/AI_Project/results/training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training history visualization saved\")"
   ],
   "metadata": {
    "id": "plot_history"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "test_loss, test_accuracy, test_precision, test_recall = cnn_model.evaluate(\n",
    "    X_test, y_test, verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Test Results:\")\n",
    "print(f\"  - Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"  - Precision: {test_precision*100:.2f}%\")\n",
    "print(f\"  - Recall: {test_recall*100:.2f}%\")\n",
    "print(f\"  - Loss: {test_loss:.4f}\")"
   ],
   "metadata": {
    "id": "evaluate_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate predictions for confusion matrix\n",
    "y_pred = cnn_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=category_names, yticklabels=category_names)\n",
    "plt.title('Confusion Matrix - Form Classification')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/AI_Project/results/confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_classes, target_names=category_names))\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ],
   "metadata": {
    "id": "confusion_matrix"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Simple User Interface\n",
    "### Test the trained model with new images"
   ],
   "metadata": {
    "id": "ui_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "def classify_uploaded_form():\n",
    "    \"\"\"Upload and classify a form image\"\"\"\n",
    "    print(\"Please upload a form image (PNG, JPG, or PDF first page)...\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        # Preprocess image\n",
    "        img = preprocess_image(filename)\n",
    "        \n",
    "        if img is not None:\n",
    "            # Add batch dimension\n",
    "            img_batch = np.expand_dims(img, axis=0)\n",
    "            \n",
    "            # Predict\n",
    "            prediction = cnn_model.predict(img_batch, verbose=0)\n",
    "            predicted_class = np.argmax(prediction[0])\n",
    "            confidence = prediction[0][predicted_class] * 100\n",
    "            \n",
    "            form_type = category_names[predicted_class]\n",
    "            \n",
    "            print(f\"\\nüéØ Prediction: {form_type}\")\n",
    "            print(f\"üìä Confidence: {confidence:.2f}%\")\n",
    "            print(f\"\\nAll probabilities:\")\n",
    "            for i, cat in enumerate(category_names):\n",
    "                print(f\"  {cat}: {prediction[0][i]*100:.2f}%\")\n",
    "            \n",
    "            # Query database for form info\n",
    "            cursor.execute(\"SELECT * FROM forms WHERE category=? LIMIT 1\", (form_type,))\n",
    "            result = cursor.fetchone()\n",
    "            \n",
    "            if result:\n",
    "                print(f\"\\nüìã Form Information:\")\n",
    "                print(f\"  Form Number: {result[1]}\")\n",
    "                print(f\"  Form Name: {result[2]}\")\n",
    "                print(f\"  Description: {result[4]}\")\n",
    "                print(f\"  Requirements: {result[5]}\")\n",
    "                print(f\"  URL: {result[3]}\")\n",
    "        else:\n",
    "            print(\"‚ùå Error: Could not process image\")\n",
    "\n",
    "# Create button to trigger upload\n",
    "upload_button = widgets.Button(\n",
    "    description='Upload & Classify Form',\n",
    "    button_style='success',\n",
    "    icon='upload'\n",
    ")\n",
    "\n",
    "def on_upload_click(b):\n",
    "    classify_uploaded_form()\n",
    "\n",
    "upload_button.on_click(on_upload_click)\n",
    "display(upload_button)"
   ],
   "metadata": {
    "id": "upload_classify"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Summary & Key Takeaways\n",
    "---\n",
    "\n",
    "## What We Built:\n",
    "1. **CNN Image Classifier** - Identifies government form types from images\n",
    "2. **SQLite Database** - Stores form information and metadata\n",
    "3. **Data Pipeline** - Augmentation, preprocessing, and splitting\n",
    "4. **Training System** - With callbacks and optimization\n",
    "\n",
    "## AI Concepts Demonstrated:\n",
    "\n",
    "### 1. **Neural Network Architecture**\n",
    "- **Input Layer**: Receives 128x128 grayscale images\n",
    "- **Convolutional Layers**: Automatically learn visual features\n",
    "- **Pooling Layers**: Reduce dimensions while preserving important info\n",
    "- **Hidden Layers (MLP)**: Combine features for classification\n",
    "- **Output Layer**: Softmax probabilities for each class\n",
    "\n",
    "### 2. **Convolutional Neural Networks (CNN)**\n",
    "- **Why Important**: Best architecture for image recognition\n",
    "- **Key Feature**: Parameter sharing across image\n",
    "- **Advantage**: Translation invariance (works even if image is shifted)\n",
    "\n",
    "### 3. **Backpropagation**\n",
    "- **Forward Pass**: Input ‚Üí layers ‚Üí prediction\n",
    "- **Loss Calculation**: Measure error with cross-entropy\n",
    "- **Backward Pass**: Calculate gradients for each weight\n",
    "- **Weight Update**: Adam optimizer adjusts weights\n",
    "\n",
    "### 4. **Loss Function (Cross-Entropy)**\n",
    "- Measures difference between prediction and truth\n",
    "- Guides backpropagation to improve model\n",
    "- Lower loss = better predictions\n",
    "\n",
    "### 5. **Training Process**\n",
    "- **Epochs**: Multiple passes through data\n",
    "- **Batches**: Process small groups at a time\n",
    "- **Callbacks**: Early stopping, checkpointing, learning rate reduction\n",
    "- **Validation**: Monitor performance on unseen data\n",
    "\n",
    "## Project Files Generated:\n",
    "- `florida_forms.db` - SQLite database\n",
    "- `best_cnn_model.h5` - Trained model\n",
    "- `X_train.npy, X_val.npy, X_test.npy` - Preprocessed images\n",
    "- `y_train.npy, y_val.npy, y_test.npy` - Labels\n",
    "- `categories.pkl` - Category names\n",
    "- `sample_forms.png` - Visualization\n",
    "- `training_history.png` - Training curves\n",
    "- `confusion_matrix.png` - Classification results\n",
    "\n",
    "## Next Steps (Week 3-5):\n",
    "1. Build MLP for text query classification\n",
    "2. Create complete user interface\n",
    "3. Integrate both models\n",
    "4. Prepare presentation and demo\n",
    "\n",
    "---\n",
    "\n",
    "**üìù Notes for Presentation:**\n",
    "- Emphasize how CNN learns features automatically\n",
    "- Explain backpropagation with training curves\n",
    "- Show confusion matrix to demonstrate accuracy\n",
    "- Demo live classification with uploaded images\n",
    "- Discuss real-world applications\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "summary"
   }
  }
 ]
}
